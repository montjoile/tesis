{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de Sentimiento a tweets en Español con el clasificador Naive Bayes\n",
    "\n",
    "#### Tweets obtenidos de base de datos con tweets recolectados en español de usuarios con geolocalizacion en Guatemala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets class\n",
    "* 0 = negativo\n",
    "* 1 = positivo\n",
    "* 2 = neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MySQLdb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieves data from db:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Retrieve tweets from db\n",
    "conn = MySQLdb.connect(\"13.58.190.139\",\"root\",\"123\",\"tesis\" )\n",
    "data = pd.read_sql(\"select * from tweets where class is not null\", conn)\n",
    "data_copy = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split label from dataset\n",
    "y = data_copy[\"class\"]\n",
    "X = data_copy[\"text\"]\n",
    "\n",
    "#Split dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5958,), (1987,), (5958,), (1987,))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import spanish stopword\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "# Spanish stemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "analyzer = CountVectorizer(stop_words = spanish_stopwords).build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spanish_stopwords) #313\n",
    "spanish_stopwords[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applies stemmer function to text\n",
    "def customized_analyzer(doc):\n",
    "    stemmed_doc = []\n",
    "    for text in doc:\n",
    "        word_list = ''\n",
    "        for word in analyzer(text):\n",
    "            item = str(stemmer.stem(word))\n",
    "            word_list = word_list + \" \" + item\n",
    "        stemmed_doc.append(word_list)\n",
    "    return stemmed_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import spanish stopword\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                lowercase = True,\n",
    "                ngram_range = (1,2),\n",
    "                stop_words = spanish_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorizar y aplicar tfidf\n",
    "X_train_counts = vectorizer.fit_transform((X_train))\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_new_counts = vectorizer.transform((X_test))\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66029189733266236"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier\n",
    "nv_classifier = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "predicted = nv_classifier.predict(X_new_tfidf)\n",
    "nv_classifier.score(X_new_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67488676396577751"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sin tfidf:\n",
    "nv_classifier_notfidf = MultinomialNB().fit(X_train_counts, y_train)\n",
    "nv_classifier_notfidf.predict(X_new_counts)\n",
    "nv_classifier_notfidf.score(X_new_counts, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Negativo       0.00      0.00      0.00       180\n",
      "   Positivo       0.52      0.31      0.39       501\n",
      "    Neutral       0.70      0.91      0.79      1306\n",
      "\n",
      "avg / total       0.59      0.67      0.62      1987\n",
      "\n",
      "[[   0   22  158]\n",
      " [   2  157  342]\n",
      " [   1  121 1184]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "expected = y_test\n",
    "predicted = nv_classifier_notfidf.predict(X_new_counts)\n",
    "print(metrics.classification_report(expected, predicted, target_names=(\"Negativo\",\"Positivo\",\"Neutral\")))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(nv_classifier_notfidf, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict new data\n",
    "new_data = [\"te amo mucho\"]\n",
    "new_vect_data = vectorizer.transform((new_data))\n",
    "nv_classifier_notfidf.predict(new_vect_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# steemer x_train x_test\n",
    "custom_vectorizer = CountVectorizer(\n",
    "                analyzer = customized_analyzer,\n",
    "                lowercase = True,\n",
    "                ngram_range = (1,2),\n",
    "                stop_words = spanish_stopwords)\n",
    "\n",
    "X_train_counts_custom = custom_vectorizer.fit_transform((X_train))\n",
    "tfidf_transformer_custom = TfidfTransformer()\n",
    "X_train_tfidf_custom = tfidf_transformer_custom.fit_transform(X_train_counts_custom)\n",
    "X_new_counts_custom = custom_vectorizer.transform((X_test))\n",
    "X_new_tfidf_custom = tfidf_transformer_custom.transform(X_new_counts_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65727226975339703"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## con steemer \n",
    "nv_steemer = MultinomialNB().fit(X_train_counts_custom, y_train)\n",
    "nv_steemer.predict(X_new_counts_custom)\n",
    "nv_steemer.score(X_new_counts_custom, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65727226975339703"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## con steemer y tfidf\n",
    "nv_steemer_tfidf = MultinomialNB().fit(X_train_tfidf_custom, y_train)\n",
    "nv_steemer_tfidf.predict(X_new_tfidf_custom)\n",
    "nv_steemer_tfidf.score(X_new_tfidf_custom, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decode Labels from predicted output\n",
    "def decode_predicted(predicted_value):\n",
    "    predict_decode = []\n",
    "    for value in predicted_value:\n",
    "        if value == 0:\n",
    "            predict_decode.append(\"Negativo\")\n",
    "        else:\n",
    "            if value == 1:\n",
    "                predict_decode.append(\"Positivo\")\n",
    "            else:\n",
    "                predict_decode.append(\"Neutral\")\n",
    "    return predict_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove index from Series\n",
    "test_tweets = X_test.reset_index()\n",
    "predict_decode = decode_predicted(predicted)\n",
    "predicted_serie = pd.Series(predict_decode, index=None)\n",
    "\n",
    "#Convert Series to DataFrame\n",
    "df = pd.DataFrame(test_tweets, columns=['text'])\n",
    "df2 = predicted_serie.to_frame(name='predicted')\n",
    "df['predicted']=df2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>AT USER feliz dia del arquitecto</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>guapa lunita</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>finaliza el recorrido del museo del ferrocarril para continuar con el bicitour esucultura URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>aveces el que no arriesga termina perdiendo mas que el que arriesga y lo pierde todo</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>a abraham dios le dijo de aqui a un ano tu imposible se hara posible jezreelgt pero abraham estaba en manre en la ubicacion de dios</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>despues de ser ministra dos por el padre solo puedo decir gracias muchas gracias senor en URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>la vida de AT USER depende de la honey monster</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>previa del proximo monday night raw ya disponible URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>el senor es mi fuerza mi escudo y fortaleza no temere porque mayor es el senor es poderoso y vencedor URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>diputados conocen resolucion de la AT USER que ordena la eleccion de los relatores contra la tortura via URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      text  \\\n",
       "1969                                                                                                     AT USER feliz dia del arquitecto    \n",
       "1970                                                                                                                         guapa lunita    \n",
       "1971                                         finaliza el recorrido del museo del ferrocarril para continuar con el bicitour esucultura URL   \n",
       "1972                                                 aveces el que no arriesga termina perdiendo mas que el que arriesga y lo pierde todo    \n",
       "1973  a abraham dios le dijo de aqui a un ano tu imposible se hara posible jezreelgt pero abraham estaba en manre en la ubicacion de dios    \n",
       "1974                                         despues de ser ministra dos por el padre solo puedo decir gracias muchas gracias senor en URL   \n",
       "1975                                                                                        la vida de AT USER depende de la honey monster   \n",
       "1976                                                                                 previa del proximo monday night raw ya disponible URL   \n",
       "1977                             el senor es mi fuerza mi escudo y fortaleza no temere porque mayor es el senor es poderoso y vencedor URL   \n",
       "1978                          diputados conocen resolucion de la AT USER que ordena la eleccion de los relatores contra la tortura via URL   \n",
       "\n",
       "     predicted  \n",
       "1969   Neutral  \n",
       "1970   Neutral  \n",
       "1971   Neutral  \n",
       "1972   Neutral  \n",
       "1973   Neutral  \n",
       "1974   Neutral  \n",
       "1975   Neutral  \n",
       "1976   Neutral  \n",
       "1977   Neutral  \n",
       "1978   Neutral  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display results\n",
    "header_style = dict(selector=\"th\", props=[('text-align', 'left')])\n",
    "pd.set_option('display.max_colwidth',140)\n",
    "df.style.set_properties(**{'text-align':'left'}).set_table_styles([header_style])\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    "### Classification code using Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words=spanish_stopwords)),\n",
    "                      #('tfidf', TfidfTransformer()), #aumento accuracy con tfidf\n",
    "                      ('clf', MultinomialNB()),])\n",
    "\n",
    "text_clf_tfidf = Pipeline([('vect', CountVectorizer(stop_words=spanish_stopwords)),\n",
    "                      ('tfidf', TfidfTransformer()), #aumento accuracy con tfidf\n",
    "                      ('clf', MultinomialNB()),])\n",
    "\n",
    "text_clf_custom = Pipeline([('vect', CountVectorizer(stop_words=spanish_stopwords, analyzer=customized_analyzer)),\n",
    "                      #('tfidf', TfidfTransformer()), #aumento accuracy con tfidf\n",
    "                      ('clf', MultinomialNB()),])\n",
    "\n",
    "text_clf_custom_tfidf = Pipeline([('vect', CountVectorizer(stop_words=spanish_stopwords, analyzer=customized_analyzer)),\n",
    "                      #('tfidf', TfidfTransformer()), #aumento accuracy con tfidf\n",
    "                      ('clf', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67388022143935578"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no tfidf\n",
    "text_clf.fit((X_train), y_train)  \n",
    "predicted = text_clf.predict((X_test))\n",
    "text_clf.score((X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66029189733266236"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf\n",
    "text_clf_tfidf.fit((X_train), y_train)  \n",
    "predicted = text_clf_tfidf.predict((X_test))\n",
    "text_clf_tfidf.score((X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65727226975339703"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom no tfdif\n",
    "text_clf_custom.fit((X_train), y_train)  \n",
    "predicted = text_clf_custom.predict((X_test))\n",
    "text_clf_custom.score((X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65727226975339703"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom  tfdif\n",
    "text_clf_custom_tfidf.fit((X_train), y_train)  \n",
    "predicted = text_clf_custom_tfidf.predict((X_test))\n",
    "text_clf_custom_tfidf.score((X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66280825364871665"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#usando customized analyzer al dataset\n",
    "text_clf_tfidf.fit(customized_analyzer(X_train), y_train)  \n",
    "predicted = text_clf_tfidf.predict(customized_analyzer(X_test))\n",
    "text_clf_tfidf.score(customized_analyzer(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    "### Probando Classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50578761952692497"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Classifier usando TDIDF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train_tfidf.toarray(), y_train).predict(X_new_tfidf.toarray())\n",
    "np.mean(y_pred == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50528434826371416"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train_counts.toarray(), y_train).predict(X_new_counts.toarray())\n",
    "np.mean(y_pred == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65727226975339703"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bernulli Naive Bayes Classifier usando TDIDF\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "y_pred =(clf.predict(X_new_tfidf))\n",
    "np.mean(y_pred == y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65727226975339703"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bernulli Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train_counts, y_train)\n",
    "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "y_pred =(clf.predict(X_new_counts))\n",
    "np.mean(y_pred == y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' camin sonr camin']\n"
     ]
    }
   ],
   "source": [
    "print(customized_analyzer([\"estoy caminando y sonriendo por el camino\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTAS: \n",
    "* Utilizar TF-IDF en texto reduce accuracy.\n",
    "* Utilizar steemr en texto reduce accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
