{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de Sentimiento a tweets en Español con el clasificador Naive Bayes\n",
    "\n",
    "#### Tweets obtenidos de base de datos con tweets recolectados en español de usuarios con geolocalizacion en Guatemala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets class\n",
    "* 0 = negativo\n",
    "* 1 = positivo\n",
    "* 2 = neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MySQLdb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieves data from db:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Retrieve tweets from db\n",
    "conn = MySQLdb.connect(\"13.58.190.139\",\"root\",\"123\",\"tesis\" )\n",
    "data = pd.read_sql(\"select * from tweets where class is not null limit 3575\", conn)\n",
    "data_copy = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split label from dataset\n",
    "y = data_copy[\"class\"]\n",
    "X = data_copy[\"text\"]\n",
    "\n",
    "#Split dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import spanish stopword\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "# Spanish stemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "analyzer = CountVectorizer(stop_words = spanish_stopwords).build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applies stemmer function to text\n",
    "def customized_analyzer(doc):\n",
    "    stemmed_doc = []\n",
    "    for text in doc:\n",
    "        word_list = ''\n",
    "        for word in analyzer(text):\n",
    "            item = str(stemmer.stem(word))\n",
    "            word_list = word_list + \" \" + item\n",
    "        stemmed_doc.append(word_list)\n",
    "    return stemmed_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import spanish stopword\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                lowercase = True,\n",
    "                ngram_range = (1,3),\n",
    "                stop_words = spanish_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bag of Words from training set\n",
    "X_train_counts = vectorizer.fit_transform((X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train classifier with TF-IDF words weigth\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier\n",
    "nv_classifier = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit classifier with test set\n",
    "X_new_counts = vectorizer.transform((X_test))\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = nv_classifier.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6767337807606264"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check accuracy\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decode Labels from predicted output\n",
    "def decode_predicted(predicted_value):\n",
    "    predict_decode = []\n",
    "    for value in predicted_value:\n",
    "        if value == 0:\n",
    "            predict_decode.append(\"Negativo\")\n",
    "        else:\n",
    "            if value == 1:\n",
    "                predict_decode.append(\"Positivo\")\n",
    "            else:\n",
    "                predict_decode.append(\"Neutral\")\n",
    "    return predict_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove index from Series\n",
    "test_tweets = X_test.reset_index()\n",
    "predict_decode = decode_predicted(predicted)\n",
    "predicted_serie = pd.Series(predict_decode, index=None)\n",
    "\n",
    "#Convert Series to DataFrame\n",
    "df = pd.DataFrame(test_tweets, columns=['text'])\n",
    "df2 = predicted_serie.to_frame(name='predicted')\n",
    "df['predicted']=df2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>sabias que guatemalacambia URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>pancho rumbo a miami con diego y cristian disfruten sus vacaciones hermanos otero AT USER URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>05 de septiembre dia del arquitect URL URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>quiero saciar mi sed con tu simiente tu esencia y tu aliento que inunde no solo mi ser que se impregne de ti tamb URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>pos me case verdad URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>si las personas fueran directas desde un inicio cuantos problemas nos ahorrariamos</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>quieren un pedazo URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>la historia de mi vida URL</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>AT USER lo sabia solo queria corroborar</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>sin duda es la respuesta que estaba esperando</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                      text  \\\n",
       "884                                                                                         sabias que guatemalacambia URL   \n",
       "885                          pancho rumbo a miami con diego y cristian disfruten sus vacaciones hermanos otero AT USER URL   \n",
       "886                                                                             05 de septiembre dia del arquitect URL URL   \n",
       "887  quiero saciar mi sed con tu simiente tu esencia y tu aliento que inunde no solo mi ser que se impregne de ti tamb URL   \n",
       "888                                                                                                 pos me case verdad URL   \n",
       "889                                    si las personas fueran directas desde un inicio cuantos problemas nos ahorrariamos    \n",
       "890                                                                                                  quieren un pedazo URL   \n",
       "891                                                                                             la historia de mi vida URL   \n",
       "892                                                                               AT USER lo sabia solo queria corroborar    \n",
       "893                                                                         sin duda es la respuesta que estaba esperando    \n",
       "\n",
       "    predicted  \n",
       "884   Neutral  \n",
       "885   Neutral  \n",
       "886   Neutral  \n",
       "887   Neutral  \n",
       "888   Neutral  \n",
       "889   Neutral  \n",
       "890   Neutral  \n",
       "891   Neutral  \n",
       "892   Neutral  \n",
       "893   Neutral  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display results\n",
    "header_style = dict(selector=\"th\", props=[('text-align', 'left')])\n",
    "pd.set_option('display.max_colwidth',140)\n",
    "df.style.set_properties(**{'text-align':'left'}).set_table_styles([header_style])\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    "### Classification code using Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words=spanish_stopwords)),\n",
    "                      #('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68232662192393734"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(customized_analyzer(X_train), y_train)  \n",
    "predicted = text_clf.predict(customized_analyzer(X_test))\n",
    "np.mean(predicted == y_test) \n",
    "\n",
    "# Accuracy NOT using stemmer function: 0.4819\n",
    "# Accuracy setting n_grams range from 1-3: 0.4819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86945169712793735"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score del classificador\n",
    "text_clf.score(customized_analyzer(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69516729,  0.7108209 ,  0.69589552,  0.66044776,  0.66728972])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print cross validation score\n",
    "scores = cross_val_score(text_clf, X_train, y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    "### Probando Classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57606263982102912"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Classifier usando TDIDF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train_tfidf.toarray(), y_train).predict(X_new_tfidf.toarray())\n",
    "np.mean(y_pred == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57606263982102912"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train_counts.toarray(), y_train).predict(X_new_counts.toarray())\n",
    "np.mean(y_pred == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66331096196868011"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bernulli Naive Bayes Classifier usando TDIDF\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "y_pred =(clf.predict(X_new_tfidf))\n",
    "np.mean(y_pred == y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66331096196868011"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bernulli Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train_counts, y_train)\n",
    "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "y_pred =(clf.predict(X_new_counts))\n",
    "np.mean(y_pred == y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTAS: \n",
    "* Utilizar TF-IDF en texto reduce accuracy.\n",
    "* Utilizar steemr en texto reduce accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
